# -*- coding: utf-8 -*-
"""HR-Employee_Attrition-Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U-yHc9Lb52qDFbGkbCJmX1khpxeyZ6op

**TCR INNOVATION Internship Task**

Team members:

1.   Maithil Deore

**Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 

import os
import matplotlib.pyplot as plt
import seaborn as sns

import sklearn
from sklearn.preprocessing import LabelEncoder,MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix

from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

from google.colab import files 
upload = files.upload()

df = pd.read_csv('HR_Employee_Attrition-1.csv')
df.head()

"""**Exploratory Data Analysis**"""

df.info()

df.shape

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

df[numeric_cols].head()

df[categorical_cols]

missing_counts = df[numeric_cols].isna().sum().sort_values(ascending=False)
missing_counts[missing_counts > 0]

missing_counts = df[categorical_cols].isna().sum().sort_values(ascending=False)
missing_counts[missing_counts > 0]

df[numeric_cols].describe()

"""** Overview**"""

df['Attrition'].value_counts()

r = df.groupby('Attrition')['Attrition'].count()
plt.pie(r, explode=[0.1, 0.1], labels=['No', 'Yes'], radius=1.5, autopct='%1.1f%%', colors=['Red', 'Green']);

df['Department'].value_counts()

df['EducationField'].value_counts()

df['JobRole'].value_counts()

df['Over18'].value_counts()

# Reassign target
df.Attrition.replace(to_replace = dict(Yes = 1, No = 0), inplace = True)

# Drop useless features
df = df.drop(columns=['StandardHours', 'EmployeeCount', 'Over18',])

"""**Data Visualization**"""

fig, axes = plt.subplots(1, 2, figsize=(16, 4))
sns.boxplot(ax = axes[0], x = df['Age'], color='Yellow')
sns.distplot(ax = axes[1],color = "Green",a=df["Age"])

fig, axes = plt.subplots(1, 2, figsize=(16, 4))
sns.boxplot(ax = axes[0],color= 'Yellow',x = df['MonthlyIncome'])
sns.distplot(ax = axes[1],color = "Green",a=df["MonthlyIncome"])

len(df[df['MonthlyIncome']>17500])

fig, axes = plt.subplots(1, 2, figsize=(16, 4))
sns.boxplot(ax = axes[0],color = "Yellow", x = df['TotalWorkingYears'])
sns.distplot(ax = axes[1],color = "Green" ,a=df["TotalWorkingYears"])

len(df[df['TotalWorkingYears']>28])

fig, axes = plt.subplots(1, 2, figsize=(16, 4))
sns.boxplot(ax = axes[0],color = "Yellow",x = df['YearsAtCompany'])
sns.distplot(ax = axes[1],color = "Green" ,a=df["YearsAtCompany"])

len(df[df['YearsAtCompany']>18])

fig, axes = plt.subplots(1, 2, figsize=(16, 4))
sns.boxplot(ax = axes[0], color = "Yellow", x = df['YearsSinceLastPromotion'])
sns.distplot(ax = axes[1],color = "Green", a=df["YearsSinceLastPromotion"])

len(df[df['YearsSinceLastPromotion']>7])

df = df[df['YearsSinceLastPromotion']<7]

len(df)

plt.figure(figsize=(20,20))
sns.heatmap(df.corr(), annot=True)

sns.countplot(x='BusinessTravel',palette=['Green', 'Yellow'], hue='Attrition', data=df);

sns.countplot(x='Department', hue='Attrition',palette=['Green', 'Yellow'], data=df);

sns.countplot(x='EducationField', hue='Attrition', palette=['Green', 'Yellow'], data=df);

sns.countplot(x='Gender', hue='Attrition', palette=['Green', 'Yellow'] ,data=df);

sns.countplot(x='JobRole', hue='Attrition', palette=['Green', 'Yellow'], data=df);

sns.countplot(x='MaritalStatus', hue='Attrition', palette=['Green', 'Yellow'], data=df);

sns.countplot(x='OverTime', hue='Attrition', palette=['Green', 'Yellow'], data=df);

"""** Preparing data for training**"""

df["Attrition"] = LabelEncoder().fit_transform(df['Attrition'])
df["BusinessTravel"] = LabelEncoder().fit_transform(df['BusinessTravel'])
df["Department"] = LabelEncoder().fit_transform(df['Department'])
df["EducationField"] = LabelEncoder().fit_transform(df['EducationField'])
df["Gender"] = LabelEncoder().fit_transform(df['Gender'])
df["JobRole"] = LabelEncoder().fit_transform(df['JobRole'])
df["MaritalStatus"] = LabelEncoder().fit_transform(df['MaritalStatus'])
df["OverTime"] = LabelEncoder().fit_transform(df['OverTime'])

numeric_cols.remove('StandardHours')
numeric_cols.remove('EmployeeCount')
df[numeric_cols] = MinMaxScaler().fit_transform(df[numeric_cols])

cols = list(df.columns)
cols.remove("Attrition")
sampled,target = SMOTE().fit_resample(df[cols],df["Attrition"])

X_train,X_test,Y_train,Y_test = train_test_split(sampled[cols], target, test_size = 0.3, shuffle=True)

"""**Model Training**"""

logistic_model = LogisticRegression(solver='liblinear',random_state=0).fit(X_train,Y_train)

print("Train Accuracy : {:.2f} %".format(accuracy_score(logistic_model.predict(X_train),Y_train)))
print("Test Accuracy : {:.2f} %".format(accuracy_score(logistic_model.predict(X_test),Y_test)))

cm = confusion_matrix(Y_test,logistic_model.predict(X_test))
classes = ["0","1"]
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)

fig, ax = plt.subplots(figsize=(10,10))
plt.title("Confusion Matrix")
disp = disp.plot(ax=ax)

plt.show()

random_forest = RandomForestClassifier(n_estimators=590, random_state=0).fit(X_train,Y_train)

print("Train Accuracy : {:.2f} %".format(accuracy_score(random_forest.predict(X_train),Y_train)))
print("Test Accuracy : {:.2f} %".format(accuracy_score(random_forest.predict(X_test),Y_test)))

cm = confusion_matrix(Y_test,random_forest.predict(X_test))
classes = ["0","1"]
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)

fig, ax = plt.subplots(figsize=(10,10))
plt.title("Confusion Matrix")
disp = disp.plot(ax=ax)

plt.show()

from xgboost import XGBClassifier
model = XGBClassifier(learning_rate=0.01,n_estimators=2000,use_label_encoder=False,random_state=420).fit(X_train,Y_train)

print("Train Accuracy : {:.2f} %".format(accuracy_score(model.predict(X_train),Y_train)))
print("Test Accuracy : {:.2f} %".format(accuracy_score(model.predict(X_test),Y_test)))

cm = confusion_matrix(Y_test,model.predict(X_test))
classes = ["0","1"]
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
fig, ax = plt.subplots(figsize=(10,10))
plt.title("Confusion Matrix")
disp = disp.plot(ax=ax)

plt.show()

"""**Conclusion**

*  Logistic Regression model has a low accuracy score of 77%


*  Random Forest Classifier performs much better having the score of 90% 
*   XGBoost model has perform with accuracy of 94%
"""